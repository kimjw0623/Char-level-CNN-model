{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "char-level cnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnjJMyFU6p2m",
        "colab_type": "code",
        "outputId": "59fa04c0-9e97-412b-c63f-4709545b712b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# write all code in one cell\n",
        "\n",
        "# ========================Load data=========================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "train_data_source = '/content/drive/My Drive/Colab Notebooks/data/ag_news_csv/train.csv'\n",
        "test_data_source = '/content/drive/My Drive/Colab Notebooks/data/ag_news_csv/test.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_data_source, header=None)\n",
        "test_df = pd.read_csv(test_data_source, header=None)\n",
        "\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
        "\n",
        "# concatenate column 1 and column 2 as one text\n",
        "for df in [train_df, test_df]:\n",
        "    df[1] = df[1] + df[2]\n",
        "    df = df.drop([2], axis=1)\n",
        "div=2\n",
        "cntdiv=0\n",
        "# convert string to lower case\n",
        "train_texts = train_df[1].values\n",
        "train_text = [s.lower() for s in train_texts]\n",
        "cc=0\n",
        "train_texts=[]\n",
        "for i in range(110000):\n",
        "  train_texts.append(train_text[i])\n",
        "\n",
        "test_texts = test_df[1].values\n",
        "test_texts = [s.lower() for s in test_texts]\n",
        "\n",
        "print(train_texts[0])\n",
        "print()\n",
        "print(test_texts[0])\n",
        "\n",
        "# =======================Convert string to index================\n",
        "# Tokenizer\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tk.fit_on_texts(train_texts)\n",
        "# If we already have a character list, then replace the tk.word_index\n",
        "# If not, just skip below part\n",
        "\n",
        "# -----------------------Skip part start--------------------------\n",
        "# construct a new vocabulary\n",
        "\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict.copy()\n",
        "# Add 'UNK' to the vocabulary\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "# -----------------------Skip part end----------------------------\n",
        "\n",
        "# Convert string to index\n",
        "train_sequences = tk.texts_to_sequences(train_texts)\n",
        "test_texts = tk.texts_to_sequences(test_texts)\n",
        "\n",
        "# Padding\n",
        "train_data = pad_sequences(train_sequences, maxlen=1014, padding='post')\n",
        "test_data = pad_sequences(test_texts, maxlen=1014, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "train_data = np.array(train_data, dtype='float32')\n",
        "test_data = np.array(test_data, dtype='float32')\n",
        "\n",
        "# =======================Get classes================\n",
        "train_classes = train_df[0].values\n",
        "train_class_list = [x - 1 for x in train_classes]\n",
        "\n",
        "test_classes = test_df[0].values\n",
        "test_class_list = [x - 1 for x in test_classes]\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "train_classes = to_categorical(train_class_list)\n",
        "test_classes = to_categorical(test_class_list)\n",
        "\n",
        "\n",
        "# =====================Char CNN=======================\n",
        "# parameter\n",
        "input_size = 1014\n",
        "vocab_size = len(tk.word_index)\n",
        "embedding_size = 69\n",
        "conv_layers = [[256, 7, 3],\n",
        "               [256, 7, 3],\n",
        "               [256, 3, -1],\n",
        "               [256, 3, -1],\n",
        "               [256, 3, -1],\n",
        "               [256, 3, 3]]\n",
        "\n",
        "fully_connected_layers = [1024, 1024]\n",
        "num_of_classes = 4\n",
        "dropout_p = 0.5\n",
        "optimizer = 'adam'\n",
        "loss = 'categorical_crossentropy'\n",
        "\n",
        "# Embedding weights\n",
        "embedding_weights = []  # (70, 69)\n",
        "embedding_weights.append(np.zeros(vocab_size))  # (0, 69)\n",
        "\n",
        "for char, i in tk.word_index.items():  # from index 1 to 69\n",
        "    onehot = np.zeros(vocab_size)\n",
        "    onehot[i - 1] = 1\n",
        "    embedding_weights.append(onehot)\n",
        "\n",
        "embedding_weights = np.array(embedding_weights)\n",
        "print('Load')\n",
        "\n",
        "# Embedding layer Initialization\n",
        "embedding_layer = Embedding(vocab_size + 1,\n",
        "                            embedding_size,\n",
        "                            input_length=input_size,\n",
        "                            weights=[embedding_weights])\n",
        "\n",
        "# Model Construction\n",
        "# Input\n",
        "inputs = Input(shape=(input_size,), name='input', dtype='int64')  # shape=(?, 1014)\n",
        "# Embedding\n",
        "x = embedding_layer(inputs)\n",
        "# Conv\n",
        "for filter_num, filter_size, pooling_size in conv_layers:\n",
        "    x = Conv1D(filter_num, filter_size)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    if pooling_size != -1:\n",
        "        x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)\n",
        "x = Flatten()(x)  # (None, 8704)\n",
        "# Fully connected layers\n",
        "for dense_size in fully_connected_layers:\n",
        "    x = Dense(dense_size, activation='relu')(x)  # dense_size == 1024\n",
        "    x = Dropout(dropout_p)(x)\n",
        "# Output Layer\n",
        "predictions = Dense(num_of_classes, activation='softmax')(x)\n",
        "# Build model\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])  # Adam, categorical_crossentropy\n",
        "model.summary()\n",
        "\n",
        "# # 1000 training samples and 100 testing samples\n",
        "# indices = np.arange(train_data.shape[0])\n",
        "# np.random.shuffle(indices)\n",
        "#\n",
        "# x_train = train_data[indices][:1000]\n",
        "# y_train = train_classes[indices][:1000]\n",
        "#\n",
        "# x_test = test_data[:100]\n",
        "# y_test = test_classes[:100]\n",
        "\n",
        "indices = np.arange(train_data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "x_train = train_data[indices]\n",
        "y_train = train_classes[indices]\n",
        "\n",
        "x_test = test_data\n",
        "y_test = test_classes\n",
        "\n",
        "# Training\n",
        "hist = model.fit(x_train, y_train,\n",
        "          validation_data=(x_test, y_test),\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          verbose=2)\n",
        "print(hist.history['val_acc'][1:])\n",
        "print(np.mean(hist.history['val_acc'][1:]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wall st. bears claw back into the black (reuters)reuters - short-sellers, wall street's dwindling\\band of ultra-cynics, are seeing green again.\n",
            "\n",
            "fears for t n pension after talksunions representing workers at turner   newall say they are 'disappointed' after talks with stricken parent firm federal mogul.\n",
            "Load\n",
            "Model: \"model_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 1014)              0         \n",
            "_________________________________________________________________\n",
            "embedding_16 (Embedding)     (None, 1014, 69)          4830      \n",
            "_________________________________________________________________\n",
            "conv1d_91 (Conv1D)           (None, 1008, 256)         123904    \n",
            "_________________________________________________________________\n",
            "activation_91 (Activation)   (None, 1008, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_46 (MaxPooling (None, 336, 256)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_92 (Conv1D)           (None, 330, 256)          459008    \n",
            "_________________________________________________________________\n",
            "activation_92 (Activation)   (None, 330, 256)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_47 (MaxPooling (None, 110, 256)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_93 (Conv1D)           (None, 108, 256)          196864    \n",
            "_________________________________________________________________\n",
            "activation_93 (Activation)   (None, 108, 256)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_94 (Conv1D)           (None, 106, 256)          196864    \n",
            "_________________________________________________________________\n",
            "activation_94 (Activation)   (None, 106, 256)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_95 (Conv1D)           (None, 104, 256)          196864    \n",
            "_________________________________________________________________\n",
            "activation_95 (Activation)   (None, 104, 256)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_96 (Conv1D)           (None, 102, 256)          196864    \n",
            "_________________________________________________________________\n",
            "activation_96 (Activation)   (None, 102, 256)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_48 (MaxPooling (None, 34, 256)           0         \n",
            "_________________________________________________________________\n",
            "flatten_16 (Flatten)         (None, 8704)              0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 1024)              8913920   \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 4)                 4100      \n",
            "=================================================================\n",
            "Total params: 11,342,818\n",
            "Trainable params: 11,342,818\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 110000 samples, validate on 7600 samples\n",
            "Epoch 1/10\n",
            " - 57s - loss: 0.8139 - acc: 0.6217 - val_loss: 0.4772 - val_acc: 0.8262\n",
            "Epoch 2/10\n",
            " - 53s - loss: 0.3458 - acc: 0.8807 - val_loss: 0.4230 - val_acc: 0.8625\n",
            "Epoch 3/10\n",
            " - 53s - loss: 0.2724 - acc: 0.9081 - val_loss: 0.3251 - val_acc: 0.8893\n",
            "Epoch 4/10\n",
            " - 53s - loss: 0.2227 - acc: 0.9240 - val_loss: 0.3184 - val_acc: 0.8951\n",
            "Epoch 5/10\n",
            " - 53s - loss: 0.1936 - acc: 0.9338 - val_loss: 0.3384 - val_acc: 0.8839\n",
            "Epoch 6/10\n",
            " - 53s - loss: 0.1604 - acc: 0.9451 - val_loss: 0.3333 - val_acc: 0.8933\n",
            "Epoch 7/10\n",
            " - 53s - loss: 0.1353 - acc: 0.9536 - val_loss: 0.3382 - val_acc: 0.8978\n",
            "Epoch 8/10\n",
            " - 53s - loss: 0.1164 - acc: 0.9603 - val_loss: 0.3813 - val_acc: 0.8888\n",
            "Epoch 9/10\n",
            " - 53s - loss: 0.0950 - acc: 0.9673 - val_loss: 0.4046 - val_acc: 0.8911\n",
            "Epoch 10/10\n",
            " - 53s - loss: 0.0865 - acc: 0.9703 - val_loss: 0.4391 - val_acc: 0.8841\n",
            "[0.8625000001254834, 0.8893421053886413, 0.8951315789473684, 0.8839473685465361, 0.8932894736842105, 0.8977631577692534, 0.8888157894736842, 0.8910526315789473, 0.8840789472429376]\n",
            "0.8873245614174513\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMsC3bJv-Lb6",
        "colab_type": "code",
        "outputId": "f55cf681-2a32-45b3-d091-aa7ac1e4e975",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}